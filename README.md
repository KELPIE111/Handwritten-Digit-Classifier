# Handwritten Digit Classifier (MNIST) + INT8 Quantization (TensorFlow Lite)

This mini‑project trains a Convolutional Neural Network (CNN) to recognize handwritten digits (0–9) from the **MNIST** dataset and then converts the model to a **fully‑quantized INT8 TensorFlow Lite** version suitable for edge / embedded deployment.

---

## Contents

- `Handwritten_Digit_Classifier_INT8_EN.ipynb` — Colab‑ready notebook:
  - data loading & preprocessing
  - CNN training
  - model evaluation
  - full INT8 post‑training quantization (weights + activations)
  - TFLite INT8 inference & quick accuracy check
  - sample predictions visualization

- `mnist_cnn_int8.tflite` — output quantized model (generated by the notebook)

---

## Requirements

Recommended environment: **Google Colab** (no setup needed).

If running locally:

- Python 3.9+
- TensorFlow 2.x
- NumPy
- Matplotlib

Install dependencies:

```bash
pip install tensorflow numpy matplotlib
```

---

## Quick Start (Google Colab)

1. Open the notebook in Colab.
2. Run cells top‑to‑bottom.
3. At the end you will get:
   - trained Keras model accuracy
   - quantized INT8 TFLite model size
   - approximate INT8 TFLite accuracy on test samples
   - visual examples of predictions

---

## Model Overview

### CNN Architecture

The notebook uses a compact CNN:

1. `Conv2D(32, 3x3) + ReLU`
2. `MaxPool(2x2)`
3. `Conv2D(64, 3x3) + ReLU`
4. `MaxPool(2x2)`
5. `Flatten`
6. `Dense(128) + ReLU`
7. `Dropout(0.3)`
8. `Dense(10) + Softmax`

This achieves ~99% accuracy on MNIST with a few epochs.

---

## INT8 Quantization (Post‑Training)

We perform **full integer quantization**:

- enable default TFLite optimizations
- provide a **representative dataset** (few hundred samples) for activation calibration
- force INT8 kernels via:
  ```python
  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
  ```
- set INT8 input/output to simplify embedded inference:
  ```python
  converter.inference_input_type = tf.int8
  converter.inference_output_type = tf.int8
  ```

### Why representative data?

TFLite needs realistic activation ranges to compute proper scales/zero‑points. Without it, accuracy may drop significantly.

---

## Running INT8 Inference

INT8 models require:

1. reading input `scale` and `zero_point`
2. quantizing input:
   ```python
   img_q = img / scale + zero_point
   img_q = np.round(img_q).astype(np.int8)
   ```
3. dequantizing output using output scale/zero‑point (optional for argmax)

The notebook provides helper `tflite_predict(img)` for convenience.

---

## Results You Should Expect

Typical outcomes:

- **Float32 Keras model**: ~0.99 test accuracy
- **INT8 TFLite model**:
  - size reduced ~3–4× vs FP32
  - accuracy very close to FP32 (usually within ~0.5%)

Exact numbers vary slightly with random initialization and training noise.

---

## Notes / Tips

- If INT8 accuracy is lower than expected, try:
  - increasing representative dataset size (e.g., 1–2k samples)
  - training a bit longer (e.g., 8–10 epochs)
- For maximum INT8 accuracy, consider **Quantization‑Aware Training (QAT)**.

---

## License / Usage

Feel free to use and modify this project for learning, demos, or as a baseline for edge ML experiments.
